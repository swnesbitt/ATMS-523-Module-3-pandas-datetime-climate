{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac34e1e4",
   "metadata": {},
   "source": [
    "# 02 — Fetch GHCN (Illinois) from NOAA S3 → Parquet\n",
    "This notebook reads **GHCN-Daily** metadata and inventory from NOAA's public S3, finds 4 Illinois stations with ≥30 years of record that **actually** have `csv/by_station` files, downloads and cleans their daily data, and writes a **local Parquet**: `data/ghcn_il_top4_daily.parquet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e8578c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "import fsspec\n",
    "\n",
    "S3_STATIONS_TXT   = \"s3://noaa-ghcn-pds/ghcnd-stations.txt\"\n",
    "S3_INVENTORY_TXT  = \"s3://noaa-ghcn-pds/ghcnd-inventory.txt\"\n",
    "S3_BY_STATION     = \"s3://noaa-ghcn-pds/csv/by_station/{id}.csv\"\n",
    "STOR = {\"anon\": True}\n",
    "\n",
    "OUTDIR = Path('../data'); OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_PARQUET = OUTDIR / 'ghcn_il_top4_daily.parquet'\n",
    "print('Output:', OUT_PARQUET.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512329ce",
   "metadata": {},
   "source": [
    "## 1) Load stations (fixed-width) and inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d911d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "colspecs = [(0,11),(12,20),(21,30),(31,37),(38,40),(41,71),(72,75),(76,79),(80,85)]\n",
    "names = ['ID','LATITUDE','LONGITUDE','ELEVATION','STATE','NAME','GSN_FLAG','HCN_CRN_FLAG','WMO_ID']\n",
    "\n",
    "stations = pd.read_fwf(S3_STATIONS_TXT, colspecs=colspecs, names=names, dtype={'ID':str,'STATE':str,'WMO_ID':str}, storage_options=STOR)\n",
    "stations['NAME'] = stations['NAME'].str.strip(); stations['STATE'] = stations['STATE'].fillna('').str.strip()\n",
    "\n",
    "inventory = pd.read_csv(\n",
    "    S3_INVENTORY_TXT, sep=r'\\s+', names=['ID','LAT','LON','ELEMENT','FIRSTYEAR','LASTYEAR'],\n",
    "    dtype={'ID':str,'ELEMENT':str,'FIRSTYEAR':int,'LASTYEAR':int}, engine='python', storage_options=STOR\n",
    ")\n",
    "\n",
    "stations.head(), inventory.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd82709",
   "metadata": {},
   "source": [
    "## 2) Compute coverage; pick IL stations with ≥30 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28f6fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage = (inventory.groupby('ID', as_index=False)\n",
    "                    .agg(first=('FIRSTYEAR','min'), last=('LASTYEAR','max'))\n",
    "                    .assign(years=lambda d: d['last'] - d['first'] + 1))\n",
    "\n",
    "il = (stations.loc[stations['STATE']=='IL', ['ID','NAME','STATE','LATITUDE','LONGITUDE','ELEVATION']]\n",
    "              .merge(coverage, on='ID', how='inner'))\n",
    "il30 = il[il['years']>=30].copy()\n",
    "il30.sort_values(['years','ID'], ascending=[False, True]).head(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d3ce3e",
   "metadata": {},
   "source": [
    "## 3) Probe `csv/by_station` and pick 4 that exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2c21e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = fsspec.filesystem('s3', **STOR)\n",
    "candidates = il30.sort_values(['years','ID'], ascending=[False, True])['ID'].tolist()\n",
    "picked, url_map = [], {}\n",
    "for sid in candidates:\n",
    "    url = S3_BY_STATION.format(id=sid)\n",
    "    if fs.exists(url):\n",
    "        picked.append(sid); url_map[sid] = url\n",
    "    if len(picked)>=4: break\n",
    "print('Picked:', picked)\n",
    "url_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5b7331",
   "metadata": {},
   "source": [
    "## 4) Load, clean, pivot to wide, convert units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a34535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_station_daily(url: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(url, storage_options=STOR, dtype={'ID':str,'ELEMENT':str}, parse_dates=['DATE'])\n",
    "    df['DATA_VALUE'] = df['DATA_VALUE'].replace(-9999, np.nan)\n",
    "    wide = (df.pivot_table(index=['ID','DATE'], columns='ELEMENT', values='DATA_VALUE', aggfunc='first').reset_index())\n",
    "    for c in ('TMAX','TMIN','TAVG'):\n",
    "        if c in wide: wide[c] = wide[c]/10.0\n",
    "    if 'PRCP' in wide: wide['PRCP'] = wide['PRCP']/10.0\n",
    "    return wide.sort_values(['ID','DATE']).reset_index(drop=True)\n",
    "\n",
    "frames = []\n",
    "for sid in picked:\n",
    "    w = load_station_daily(url_map[sid])\n",
    "    frames.append(w); print(sid, w.shape)\n",
    "\n",
    "daily = pd.concat(frames, ignore_index=True)\n",
    "daily.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eba43c9",
   "metadata": {},
   "source": [
    "## 5) Write Parquet and quick verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92beca9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_cols = [c for c in ['ID','DATE','PRCP','TMAX','TMIN','TAVG','SNOW','SNWD'] if c in daily.columns]\n",
    "daily = daily[first_cols + [c for c in daily.columns if c not in first_cols]]\n",
    "daily.to_parquet(OUT_PARQUET, index=False)\n",
    "print('Wrote:', OUT_PARQUET.resolve())\n",
    "pd.read_parquet(OUT_PARQUET).groupby('ID').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5220d0e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xarray-climate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
